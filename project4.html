<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Multimodal Deep Learning for VQA</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background: #f9f9f9;
      color: #333;
    }

    .container {
      max-width: 1000px;
      margin: 20px auto;
      padding: 20px;
      background: #fff;
      box-shadow: 0 8px 16px rgba(0, 0, 0, 0.1);
      border-radius: 8px;
    }

    h1 {
      color: #1E90FF;
      font-size: 28px;
    }

    p {
      line-height: 1.6;
    }

    .navigation-buttons {
      display: flex;
      justify-content: space-between;
      margin-bottom: 20px;
      flex-wrap: wrap;
    }

    .navigation-buttons button {
      background-color: #1E90FF;
      color: white;
      padding: 10px 20px;
      border: none;
      font-size: 1rem;
      border-radius: 4px;
      cursor: pointer;
      margin: 5px 0;
      transition: background 0.3s;
    }

    .navigation-buttons button:hover {
      background-color: #187bcd;
    }

    .slideshow-container {
      max-width: 100%;
      position: relative;
      margin: 40px auto;
    }

    .mySlides {
      display: none;
      text-align: center;
    }

    .mySlides img {
      width: 100%;
      max-height: 600px;
      object-fit: contain;
      border-radius: 8px;
    }

    .text {
      font-size: 14px;
      color: #000;
      padding: 8px;
      background: #f1f1f1;
      border-radius: 4px;
      margin-top: 5px;
    }

    .numbertext {
      position: absolute;
      top: 0;
      left: 0;
      color: #000;
      background: rgba(255, 255, 255, 0.8);
      padding: 6px 10px;
      border-radius: 0 0 8px 0;
      font-size: 13px;
    }

    .dot-container {
      text-align: center;
      margin-top: 10px;
    }

    .dot {
      height: 12px;
      width: 12px;
      margin: 0 4px;
      background-color: #bbb;
      border-radius: 50%;
      display: inline-block;
      transition: background-color 0.6s ease;
    }

    .active {
      background-color: #717171;
    }

    @keyframes fade {
      from {opacity: .4}
      to {opacity: 1}
    }

    @media (max-width: 768px) {
      .navigation-buttons {
        flex-direction: column;
        align-items: stretch;
      }

      .navigation-buttons button {
        width: 100%;
      }

      h1 {
        font-size: 22px;
      }
    }
  </style>
</head>
<body>
  <div class="container">
    <div class="navigation-buttons">
      <button onclick="window.location.href='./project3.html'"><i class="fa fa-arrow-left"></i> Prev Project</button>
      <button onclick="window.location.href='./index.html'"><i class="fa fa-home"></i> Home</button>
      <button onclick="window.location.href='./project5.html'">Next Project <i class="fa fa-arrow-right"></i></button>
    </div>

    <header>
      <h1>Multimodal Deep Learning for Visual Question Answering</h1>
      <p>
        Visual Question Answering (VQA) is a deep learning task where a model is given an image and a related question and must generate a correct answer based on the information in the image. The challenge lies in the model's ability to understand both the visual content and the semantics of the question, requiring it to implicitly reason about the relationships between objects in the image and the details asked in the question. This task is more complex than simple image classification, as the model must comprehend the image and answer specific questions related to its contents.<br> <br>

        To tackle this, we built a transformer-based model, which has proven effective in handling tasks that require understanding both images and text. Transformers, particularly those utilizing attention mechanisms, are capable of processing complex relationships between visual elements and textual queries, making them well-suited for VQA. We trained our model using the **Neural Symbolic VQA** dataset, a state-of-the-art dataset designed for this task, which includes a wide range of questions and images that challenge the modelâ€™s ability to reason and understand context.<br> <br>
        
        Our model achieved a **maximum accuracy of 70%** on the dataset, which is promising given the challenges of VQA tasks. However, due to time constraints, the model was not fully trained, and there is potential for further improvements with more training data and fine-tuning. While our results are encouraging, we recognize that VQA models still face difficulties with complex reasoning tasks, and future work could focus on improving multi-step reasoning and handling more intricate questions.<br> <br>
        
        Despite these challenges, our work highlights the potential of transformer-based models for VQA tasks and opens the door to applications in areas such as human-computer interaction, robotics, and automated content generation. With further advancements in model architectures and training techniques, VQA systems are expected to become more accurate and effective in interpreting and answering questions about images.
            </p>
    </header>

    <div class="slideshow-container">
      <div class="mySlides fade">
        <div class="numbertext">1 / 8</div>
        <img src="images/project4/dataset_pie.png" alt="Dataset split">
        <div class="text">The split of the data</div>
      </div>

      <div class="mySlides fade">
        <div class="numbertext">2 / 8</div>
        <img src="images/project4/model.png" alt="Original model">
        <div class="text">The original model proposed in the paper</div>
      </div>

      <div class="mySlides fade">
        <div class="numbertext">3 / 8</div>
        <img src="images/project4/ourmodel.png" alt="Modified model">
        <div class="text">The modified model we built</div>
      </div>

      <div class="mySlides fade">
        <div class="numbertext">4 / 8</div>
        <img src="images/project4/results1.png" alt="Qualitative results 1">
        <div class="text">Qualitative results</div>
      </div>

      <div class="mySlides fade">
        <div class="numbertext">5 / 8</div>
        <img src="images/project4/results2.png" alt="Qualitative results 2">
        <div class="text">Qualitative results</div>
      </div>

      <div class="mySlides fade">
        <div class="numbertext">6 / 8</div>
        <img src="images/project4/results3.png" alt="Qualitative results 3">
        <div class="text">Qualitative results</div>
      </div>

      <div class="mySlides fade">
        <div class="numbertext">7 / 8</div>
        <img src="images/project4/results4.png" alt="Qualitative results 4">
        <div class="text">Qualitative results</div>
      </div>

      <div class="mySlides fade">
        <div class="numbertext">8 / 8</div>
        <img src="images/project4/results5.png" alt="Qualitative results 5">
        <div class="text">Qualitative results</div>
      </div>
    </div>

    <div class="dot-container">
      <span class="dot"></span> 
      <span class="dot"></span> 
      <span class="dot"></span> 
      <span class="dot"></span> 
      <span class="dot"></span> 
      <span class="dot"></span> 
      <span class="dot"></span> 
      <span class="dot"></span>
    </div>
  </div>

  <script>
    let slideIndex = 0;
    const slideTimes = [3000, 3000, 3000, 3000, 3000, 3000, 3000, 3000];
    const slides = document.getElementsByClassName("mySlides");
    const dots = document.getElementsByClassName("dot");

    function showSlides() {
      for (let i = 0; i < slides.length; i++) {
        slides[i].style.display = "none";
        dots[i].className = dots[i].className.replace(" active", "");
      }
      slideIndex++;
      if (slideIndex > slides.length) { slideIndex = 1; }
      slides[slideIndex - 1].style.display = "block";
      dots[slideIndex - 1].className += " active";
      setTimeout(showSlides, slideTimes[slideIndex - 1]);
    }

    showSlides();
  </script>
</body>
</html>